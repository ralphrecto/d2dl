{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "from livelossplot import PlotLosses"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "EMBEDDING_DIM = 256\n",
    "BATCH_SIZE = 8\n",
    "CONTEXT_LEN = 128\n",
    "TEXT_FILE = \"./datasets/tinyshakespeare.txt\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt2\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "with open(TEXT_FILE, \"r\") as f:\n",
    "    data = f.read() "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "tokenized_data = torch.tensor(tokenizer.encode(data))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# partitioning scheme from \n",
    "# https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#partitioning-sequences\n",
    "d = torch.randint(CONTEXT_LEN, size = (1,))[0].item()\n",
    "\n",
    "# discard last item which may be of diff size\n",
    "token_partitions_X = torch.stack(torch.split(tokenized_data[d:], CONTEXT_LEN)[:-1])\n",
    "token_partitions_Y = torch.stack(torch.split(tokenized_data[d+1:], CONTEXT_LEN)[:-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, ctx_len, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # TODO autoregressive mask; generalize this\n",
    "        mask = torch.tril(torch.ones(ctx_len, ctx_len))\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def _masked_softmax(self, logits, mask):\n",
    "        logits.masked_fill_(mask == 0, float('-inf'))\n",
    "        return self.softmax(logits)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        assert queries.shape == keys.shape\n",
    "        assert queries.shape[1] == values.shape[1]\n",
    "\n",
    "        qk_dim = queries.shape[2]\n",
    "\n",
    "        # transpose (ctx, embedding) dims\n",
    "        scaled_dot_prod = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(qk_dim)\n",
    "\n",
    "        mask = self.get_buffer('mask')\n",
    "        attention_weights = self._masked_softmax(scaled_dot_prod, mask)\n",
    "\n",
    "        # The book uses dropout for weights but doesn't explain why that specifically?\n",
    "        # https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#scaled-dot-product-attention\n",
    "        return torch.bmm(self.dropout(attention_weights), values)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "source": [
    "class MultiHeadAttention1(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, qkv_dim, ctx_len, dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        assert qkv_dim % num_heads == 0\n",
    "\n",
    "        self.qkv_dim = qkv_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            Attention(ctx_len, dropout = dropout)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        self.q_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.k_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.v_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.output_layer = nn.LazyLinear(qkv_dim)\n",
    "\n",
    "        # TODO autoregressive mask; generalize this\n",
    "        mask = torch.tril(torch.ones(ctx_len, ctx_len))\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def _masked_softmax(self, logits, mask):\n",
    "        logits.masked_fill_(mask == 0, float('-inf'))\n",
    "        return self.softmax(logits)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        assert queries.shape == keys.shape\n",
    "        assert queries.shape == values.shape\n",
    "        assert queries.shape[-1] == self.qkv_dim\n",
    "\n",
    "        head_dim = self.qkv_dim // self.num_heads\n",
    "\n",
    "        # TODO these are just truncated in the book?!\n",
    "        # https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#implementation\n",
    "        # why not use a linear layer to learn a lower dim value?\n",
    "        pq = torch.split(self.q_linear(queries), head_dim, dim=-1)\n",
    "        pk = torch.split(self.k_linear(keys), head_dim, dim=-1)\n",
    "        pv = torch.split(self.v_linear(values), head_dim, dim=-1)\n",
    "\n",
    "        # TODO use transpose trick to parallelize across heads\n",
    "        head_vals = [\n",
    "            self.attention_heads[i](pq[i], pk[i], pv[i])\n",
    "            for i in range(self.num_heads)\n",
    "        ]\n",
    "\n",
    "        cat_head_vals = torch.cat(head_vals, dim=-1)\n",
    "        return self.dropout(self.output_layer(cat_head_vals))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "source": [
    "class MultiHeadAttention2(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, qkv_dim, ctx_len, dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        assert qkv_dim % num_heads == 0\n",
    "\n",
    "        self.qkv_dim = qkv_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # self.attention_heads = nn.ModuleList([\n",
    "        #     Attention(ctx_len, dropout = dropout)\n",
    "        #     for _ in range(num_heads)\n",
    "        # ])\n",
    "\n",
    "        self.q_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.k_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.v_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.output_layer = nn.LazyLinear(qkv_dim)\n",
    "\n",
    "        # TODO autoregressive mask; generalize this\n",
    "        mask = torch.tril(torch.ones(ctx_len, ctx_len))\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def _masked_softmax(self, logits, mask):\n",
    "        logits.masked_fill_(mask == 0, float('-inf'))\n",
    "        return self.softmax(logits)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        assert queries.shape == keys.shape\n",
    "        assert queries.shape == values.shape\n",
    "        assert queries.shape[-1] == self.qkv_dim\n",
    "\n",
    "        # # TODO these are just truncated in the book?!\n",
    "        # # https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#implementation\n",
    "        # # why not use a linear layer to learn a lower dim value?\n",
    "        # pq = torch.split(self.q_linear(queries), head_dim, dim=-1)\n",
    "        # pk = torch.split(self.k_linear(keys), head_dim, dim=-1)\n",
    "        # pv = torch.split(self.v_linear(values), head_dim, dim=-1)\n",
    "\n",
    "        # # TODO use transpose trick to parallelize across heads\n",
    "        # head_vals = [\n",
    "        #     self.attention_heads[i](pq[i], pk[i], pv[i])\n",
    "        #     for i in range(self.num_heads)\n",
    "        # ]\n",
    "\n",
    "        # cat_head_vals = torch.cat(head_vals, dim=-1)\n",
    "        # return self.dropout(self.output_layer(cat_head_vals))\n",
    "\n",
    "        batch_size = queries.shape[0]\n",
    "\n",
    "        d_k = self.qkv_dim // self.num_heads\n",
    "        q = self.q_linear(queries).view(batch_size, -1, self.num_heads, d_k).transpose(1, 2)\n",
    "        k = self.k_linear(keys).view(batch_size, -1, self.num_heads, d_k).transpose(1, 2)\n",
    "        v = self.v_linear(values).view(batch_size, -1, self.num_heads, d_k).transpose(1, 2)\n",
    "\n",
    "        # Attention mechanism\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        mask = self.get_buffer('mask')\n",
    "        attn = self._masked_softmax(scores, mask)\n",
    "\n",
    "        # The book uses dropout for weights but doesn't explain why just that specifically?\n",
    "        # https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#scaled-dot-product-attention\n",
    "        output = (\n",
    "            torch.matmul(self.dropout(attn), v)\n",
    "            .transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, -1, self.qkv_dim)\n",
    "        )\n",
    "\n",
    "        return output\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "source": [
    "class PositionwiseFFN(nn.Sequential):\n",
    "    def __init__(self, ffn_hidden_dim: int, ffn_output_dim: int):\n",
    "        super().__init__(\n",
    "            nn.LazyLinear(ffn_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(ffn_output_dim)\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "source": [
    "class AddNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, norm_shape, dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(norm_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(X + self.dropout(Y))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads: int, model_dim: int, ffn_hidden_dim: int, ctx_len: int, dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention1(num_heads, model_dim, ctx_len, dropout)\n",
    "        self.add_norm1 = AddNorm(model_dim, dropout)\n",
    "        self.ffn = PositionwiseFFN(ffn_hidden_dim, model_dim)\n",
    "        self.add_norm2 = AddNorm(model_dim, dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        attention = self.attention(X, X, X)\n",
    "        resid_out = self.add_norm1(X, attention)\n",
    "\n",
    "        ffn_out = self.ffn(resid_out)\n",
    "        return self.add_norm2(resid_out, ffn_out)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "source": [
    "class RalphGPT(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks: int,\n",
    "        num_heads: int,\n",
    "        model_dim: int,\n",
    "        ffn_hidden_dim: int,\n",
    "        vocab_size: int,\n",
    "        context_len: int,\n",
    "        dropout = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_len = context_len\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        # TODO do we need a max_norm? seems like this would be important\n",
    "        # depending on positional embedding scheme\n",
    "        self.token_embedder = nn.Embedding(\n",
    "            num_embeddings = vocab_size,\n",
    "            embedding_dim = model_dim\n",
    "        )\n",
    "\n",
    "        # use learned positional embeddings for simplicity\n",
    "        # TODO what are the tradeoffs with fixed positional embeddings besides less storage?\n",
    "        self.positional_embedder = nn.Embedding(\n",
    "            num_embeddings = context_len,\n",
    "            embedding_dim = model_dim\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(*[\n",
    "            TransformerDecoderBlock(num_heads, model_dim, ffn_hidden_dim, context_len, dropout)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.output = nn.LazyLinear(vocab_size)\n",
    "\n",
    "        context_idx_tensor = torch.tensor(list(range(context_len)))\n",
    "        self.register_buffer('context_idx_tensor', context_idx_tensor)\n",
    "\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens.shape is (batch size, ctx len)\n",
    "        assert tokens.shape[-1] == self.context_len\n",
    "\n",
    "        context_idx_tensor = self.get_buffer('context_idx_tensor')\n",
    "        positional_embeddings = self.positional_embedder(context_idx_tensor)\n",
    "\n",
    "        token_embeddings = self.token_embedder(tokens)\n",
    "        embeddings = token_embeddings + positional_embeddings\n",
    "\n",
    "        return self.output(self.decoder(embeddings))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "source": [
    "ralphgpt = RalphGPT(\n",
    "    num_blocks = 10,\n",
    "    num_heads = 4,\n",
    "    model_dim = EMBEDDING_DIM,\n",
    "    ffn_hidden_dim = EMBEDDING_DIM,\n",
    "    vocab_size = tokenizer.n_vocab,\n",
    "    context_len = CONTEXT_LEN,\n",
    "    dropout = 0.1\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ralph/.venvs/nanogpt/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "source": [
    "ralphgpt.to(device)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RalphGPT(\n",
       "  (token_embedder): Embedding(50257, 256)\n",
       "  (positional_embedder): Embedding(128, 256)\n",
       "  (decoder): Sequential(\n",
       "    (0): TransformerDecoderBlock(\n",
       "      (attention): MultiHeadAttention1(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (attention_heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (q_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (k_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (v_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (output_layer): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (add_norm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionwiseFFN(\n",
       "        (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerDecoderBlock(\n",
       "      (attention): MultiHeadAttention1(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (attention_heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (q_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (k_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (v_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (output_layer): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (add_norm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionwiseFFN(\n",
       "        (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerDecoderBlock(\n",
       "      (attention): MultiHeadAttention1(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (attention_heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (q_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (k_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (v_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (output_layer): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (add_norm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionwiseFFN(\n",
       "        (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerDecoderBlock(\n",
       "      (attention): MultiHeadAttention1(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (attention_heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (q_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (k_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (v_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (output_layer): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (add_norm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionwiseFFN(\n",
       "        (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerDecoderBlock(\n",
       "      (attention): MultiHeadAttention1(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (attention_heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (q_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (k_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (v_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (output_layer): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (add_norm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionwiseFFN(\n",
       "        (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerDecoderBlock(\n",
       "      (attention): MultiHeadAttention1(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (attention_heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (q_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (k_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (v_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (output_layer): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (add_norm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionwiseFFN(\n",
       "        (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerDecoderBlock(\n",
       "      (attention): MultiHeadAttention1(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (attention_heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (q_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (k_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (v_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (output_layer): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (add_norm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionwiseFFN(\n",
       "        (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerDecoderBlock(\n",
       "      (attention): MultiHeadAttention1(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (attention_heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (q_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (k_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (v_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (output_layer): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (add_norm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionwiseFFN(\n",
       "        (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerDecoderBlock(\n",
       "      (attention): MultiHeadAttention1(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (attention_heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (q_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (k_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (v_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (output_layer): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (add_norm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionwiseFFN(\n",
       "        (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerDecoderBlock(\n",
       "      (attention): MultiHeadAttention1(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (attention_heads): ModuleList(\n",
       "          (0-3): 4 x Attention(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (q_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (k_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (v_linear): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (output_layer): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (add_norm1): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ffn): PositionwiseFFN(\n",
       "        (0): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm2): AddNorm(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output): LazyLinear(in_features=0, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 387
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "source": [
    "batches_x = torch.split(token_partitions_X.to(device), BATCH_SIZE, dim = 0)\n",
    "batches_y = torch.split(token_partitions_Y.to(device), BATCH_SIZE, dim = 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(ralphgpt.parameters(), lr=0.0005)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "source": [
    "num_epochs = 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "source": [
    "gradients = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "source": [
    "model = ralphgpt\n",
    "liveloss = PlotLosses()\n",
    "for epoch in range(num_epochs):\n",
    "    batches = list(zip(batches_x, batches_y))\n",
    "    random.shuffle(batches)\n",
    "\n",
    "    losses = []\n",
    "    for batch_i, batch in enumerate(batches):\n",
    "        batch_x, batch_y = batch\n",
    "\n",
    "        pred_y = model(batch_x)\n",
    "\n",
    "        flat_pred_y = torch.flatten(pred_y, 0, 1)\n",
    "        flat_batch_y = torch.flatten(batch_y, 0, 1)\n",
    "\n",
    "        loss = loss_fn(flat_pred_y, flat_batch_y)\n",
    "        loss_val = loss.item()\n",
    "\n",
    "        losses.append(loss_val)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # # Vector to store each gradient's L2 norm squared\n",
    "        # grads = []\n",
    "\n",
    "        # for parameter in model.parameters():\n",
    "        #     if parameter.requires_grad and parameter.grad is not None:\n",
    "        #         # Add the squared L2 norm of each gradient to the list\n",
    "        #         grads.append(parameter.grad.data.norm(2).item() ** 2)\n",
    "\n",
    "        # # Sum up the squared norms and take the square root to get the total L2 norm\n",
    "        # total_grad_norm = torch.sqrt(torch.tensor(grads).sum())\n",
    "        # gradients.append(total_grad_norm)\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    mean_loss = pd.Series(losses).mean()\n",
    "    logs = dict(\n",
    "        loss = mean_loss\n",
    "    )\n",
    "    liveloss.update(logs)\n",
    "    liveloss.send()\n",
    "\n",
    "    print(f\"[epoch: {epoch}] loss\", loss_val)\n",
    "        # val_pred_y = self.model(val_X)\n",
    "        # val_loss = self.loss_fn(val_pred_y, val_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "source": [
    "def pad_input(s: str):\n",
    "    tokens = tokenizer.encode(s)\n",
    "\n",
    "    pad_len = CONTEXT_LEN - len(tokens)\n",
    "\n",
    "    if pad_len < 0:\n",
    "        raise Exception(f\"Got {len(tokens)} tokens but context len is {CONTEXT_LEN}\")\n",
    "\n",
    "    padding = pad_len * [0]\n",
    "\n",
    "    return tokens + padding"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "source": [
    "def infer(model, s, num_out = 50):\n",
    "\n",
    "    input_tokens = pad_input(s)\n",
    "    output_tokens = []\n",
    "\n",
    "    for _ in range(num_out):\n",
    "        batch = torch.tensor(input_tokens).unsqueeze(0).to(device)\n",
    "\n",
    "        logits = model(batch)\n",
    "        tokens = logits.argmax(-1).squeeze(0).tolist()\n",
    "\n",
    "        pred_token = tokens[-1]\n",
    "        output_tokens.append(pred_token)\n",
    "        input_tokens = input_tokens[:-1]\n",
    "        input_tokens.append(pred_token)\n",
    "\n",
    "\n",
    "    return tokenizer.decode(output_tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "source": [
    "print(infer(model, \"For all the world's a stage\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " this day the trump day your trump back back back on Thursday ho devil back back, not a word hours am back again are Thursday ho gods th are Thursday am noise we'll be a canopy back, be gods are you say this hard Thursday Henry am\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('nanogpt': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "interpreter": {
   "hash": "b76fb5d7fb166d469bca8f05fa9f70abccb633edd5cdd7db479ed33a47fa1881"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}