{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "BATCH_SIZE = 8\n",
    "CONTEXT_LEN = 1024\n",
    "TEXT_FILE = \"./datasets/tinyshakespeare.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEXT_FILE, \"r\") as f:\n",
    "    data = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = torch.tensor(tokenizer.encode(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning scheme from \n",
    "# https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#partitioning-sequences\n",
    "d = torch.randint(CONTEXT_LEN, size = (1,))[0].item()\n",
    "\n",
    "# discard last item which may be of diff size\n",
    "token_partitons = torch.stack(torch.split(tokenized_data[d:], CONTEXT_LEN)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = token_partitons[:BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do we need a max_norm? seems like this would be important\n",
    "# depending on positional embedding scheme\n",
    "embedder = nn.Embedding(\n",
    "    num_embeddings = tokenizer.n_vocab,\n",
    "    embedding_dim = EMBEDDING_DIM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use learned positional embeddings for simplicity\n",
    "# TODO what are the tradeoffs with fixed positional embeddings besides less storage?\n",
    "positional_embedder = nn.Embedding(\n",
    "    num_embeddings = CONTEXT_LEN,\n",
    "    embedding_dim = EMBEDDING_DIM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_idx_tensor = torch.tensor(list(range(CONTEXT_LEN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_embeddings = positional_embedder(context_idx_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = embedder(sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = token_embeddings + positional_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries, keys, values = embeddings, embeddings, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert queries.shape == keys.shape\n",
    "assert keys.shape == values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate a mask for subsequent positions (upper triangular mask).\"\"\"\n",
    "subsequent_mask = torch.triu(torch.ones(), diagonal=1).type(torch.bool)\n",
    "return subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose (ctx, embedding) dims\n",
    "scaled_dot_prod = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ninf_mask = torch.triu(torch.ones(scaled_dot_prod.shape[1:]), diagonal=1) * float('-inf')\n",
    "ninf_mask[torch.isnan(ninf_mask)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ralph/.venvs/d2dl/lib/python3.9/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = softmax(ninf_mask + scaled_dot_prod)\n",
    "attention_weights[torch.isnan(attention_weights)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The book uses dropout for weights but doesn't explain why that specifically?\n",
    "# https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#scaled-dot-product-attention\n",
    "attention = torch.bmm(dropout(attention_weights), values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
