{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "BATCH_SIZE = 8\n",
    "CONTEXT_LEN = 128\n",
    "TEXT_FILE = \"./datasets/tinyshakespeare.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEXT_FILE, \"r\") as f:\n",
    "    data = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = torch.tensor(tokenizer.encode(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning scheme from \n",
    "# https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#partitioning-sequences\n",
    "d = torch.randint(CONTEXT_LEN, size = (1,))[0].item()\n",
    "\n",
    "# discard last item which may be of diff size\n",
    "token_partitions_X = torch.stack(torch.split(tokenized_data[d:], CONTEXT_LEN)[:-1])\n",
    "token_partitions_Y = torch.stack(torch.split(tokenized_data[d+1:], CONTEXT_LEN)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = token_partitions_X[:BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do we need a max_norm? seems like this would be important\n",
    "# depending on positional embedding scheme\n",
    "embedder = nn.Embedding(\n",
    "    num_embeddings = tokenizer.n_vocab,\n",
    "    embedding_dim = EMBEDDING_DIM\n",
    ")\n",
    "# use learned positional embeddings for simplicity\n",
    "# TODO what are the tradeoffs with fixed positional embeddings besides less storage?\n",
    "positional_embedder = nn.Embedding(\n",
    "    num_embeddings = CONTEXT_LEN,\n",
    "    embedding_dim = EMBEDDING_DIM\n",
    ")\n",
    "context_idx_tensor = torch.tensor(list(range(CONTEXT_LEN)))\n",
    "positional_embeddings = positional_embedder(context_idx_tensor)\n",
    "token_embeddings = embedder(sample_batch)\n",
    "embeddings = token_embeddings + positional_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _masked_softmax(self, logits, mask):\n",
    "        logits.masked_fill_(mask == 0, float('-inf'))\n",
    "        return self.softmax(logits)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        assert queries.shape == keys.shape\n",
    "        assert queries.shape[1] == values.shape[1]\n",
    "\n",
    "        ctx_len = queries.shape[1]\n",
    "        qk_dim = queries.shape[2]\n",
    "\n",
    "        # transpose (ctx, embedding) dims\n",
    "        scaled_dot_prod = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(qk_dim)\n",
    "\n",
    "        # TODO autoregressive mask; generalize this\n",
    "        mask = torch.tril(torch.ones(ctx_len, ctx_len))  \n",
    "\n",
    "        attention_weights = self._masked_softmax(scaled_dot_prod, mask)\n",
    "\n",
    "        # The book uses dropout for weights but doesn't explain why that specifically?\n",
    "        # https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#scaled-dot-product-attention\n",
    "        return torch.bmm(self.dropout(attention_weights), values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, qkv_dim, dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        assert qkv_dim % num_heads == 0\n",
    "\n",
    "        self.qkv_dim = qkv_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention_heads = [\n",
    "            Attention(dropout = dropout)\n",
    "            for _ in range(num_heads)\n",
    "        ]\n",
    "\n",
    "        self.q_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.k_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.v_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.output_layer = nn.LazyLinear(qkv_dim)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        assert queries.shape == keys.shape\n",
    "        assert queries.shape == values.shape\n",
    "        assert queries.shape[-1] == self.qkv_dim\n",
    "\n",
    "        head_dim = self.qkv_dim // self.num_heads\n",
    "        # TODO these are just truncated in the book?!\n",
    "        # https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#implementation\n",
    "        # why not use a linear layer to learn a lower dim value?\n",
    "        pq = torch.split(self.q_linear(queries), head_dim, dim=-1)\n",
    "        pk = torch.split(self.k_linear(keys), head_dim, dim=-1)\n",
    "        pv = torch.split(self.v_linear(values), head_dim, dim=-1)\n",
    "\n",
    "        # TODO use transpose trick to parallelize across heads\n",
    "        head_vals = [\n",
    "            self.attention_heads[i](pq[i], pk[i], pv[i])\n",
    "            for i in range(self.num_heads)\n",
    "        ]\n",
    "\n",
    "        cat_head_vals = torch.cat(head_vals, dim=-1)\n",
    "        return self.output_layer(cat_head_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Sequential):\n",
    "    def __init__(self, ffn_hidden_dim: int, ffn_output_dim: int):\n",
    "        super().__init__(\n",
    "            nn.LazyLinear(ffn_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(ffn_output_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, norm_shape, dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(norm_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(X + self.dropout(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads: int, model_dim: int, ffn_hidden_dim: int, dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(num_heads, model_dim, dropout)\n",
    "        self.add_norm1 = AddNorm(model_dim, dropout)\n",
    "        self.ffn = PositionwiseFFN(ffn_hidden_dim, model_dim)\n",
    "        self.add_norm2 = AddNorm(model_dim, dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        attention = self.attention(X, X, X)\n",
    "        resid_out = self.add_norm1(X, attention)\n",
    "\n",
    "        ffn_out = self.ffn(resid_out)\n",
    "        return self.add_norm2(resid_out, ffn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RalphGPT(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks: int,\n",
    "        num_heads: int,\n",
    "        model_dim: int,\n",
    "        ffn_hidden_dim: int,\n",
    "        vocab_size: int,\n",
    "        context_len: int,\n",
    "        dropout = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_len = context_len\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        # TODO do we need a max_norm? seems like this would be important\n",
    "        # depending on positional embedding scheme\n",
    "        self.token_embedder = nn.Embedding(\n",
    "            num_embeddings = vocab_size,\n",
    "            embedding_dim = model_dim\n",
    "        )\n",
    "\n",
    "        # use learned positional embeddings for simplicity\n",
    "        # TODO what are the tradeoffs with fixed positional embeddings besides less storage?\n",
    "        self.positional_embedder = nn.Embedding(\n",
    "            num_embeddings = context_len,\n",
    "            embedding_dim = model_dim\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(*[\n",
    "            TransformerDecoderBlock(num_heads, model_dim, ffn_hidden_dim, dropout)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.output = nn.LazyLinear(vocab_size)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens.shape is (batch size, ctx len)\n",
    "        assert tokens.shape[-1] == self.context_len\n",
    "\n",
    "        context_idx_tensor = torch.tensor(list(range(self.context_len)))\n",
    "        positional_embeddings = self.positional_embedder(context_idx_tensor)\n",
    "\n",
    "        token_embeddings = embedder(tokens)\n",
    "        embeddings = token_embeddings + positional_embeddings\n",
    "\n",
    "        return self.output(self.decoder(embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "ralphgpt = RalphGPT(\n",
    "    num_blocks = 10,\n",
    "    num_heads = 4,\n",
    "    model_dim = EMBEDDING_DIM,\n",
    "    ffn_hidden_dim = EMBEDDING_DIM,\n",
    "    vocab_size = tokenizer.n_vocab,\n",
    "    context_len = CONTEXT_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_x = torch.split(token_partitions_X, BATCH_SIZE, dim = 0)\n",
    "batches_y = torch.split(token_partitions_Y, BATCH_SIZE, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.SGD(ralphgpt.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    batches = list(zip(batches_x, batches_y))\n",
    "    random.shuffle(batches)\n",
    "\n",
    "    for batch_i, batch in enumerate(batches):\n",
    "        batch_x, batch_y = batch\n",
    "        pred_y = ralphgpt(batch_x)\n",
    "\n",
    "        flat_pred_y = torch.flatten(pred_y, 0, 1)\n",
    "        flat_batch_y = torch.flatten(batch_y, 0, 1)\n",
    "\n",
    "        loss = loss_fn(flat_pred_y, flat_batch_y)\n",
    "        print(f\"[epoch: {epoch}][batch: {batch_i}] loss\", loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        # val_pred_y = self.model(val_X)\n",
    "        # val_loss = self.loss_fn(val_pred_y, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, plot_cadence):\n",
    "    train_dataloader, val_dataloader = self.dataset.get_dataloaders(self.hyperparams.general[\"batch_size\"])\n",
    "\n",
    "    print(train_dataloader)\n",
    "    train_loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    for epoch_num in range(self.hyperparams.general[\"num_epochs\"]):\n",
    "        for batch_num, (train_data, val_data) in enumerate(zip(train_dataloader, val_dataloader)):\n",
    "            train_X, train_y = train_data\n",
    "            val_X, val_y = val_data\n",
    "\n",
    "            pred_y = self.model(train_X)\n",
    "            loss = self.loss_fn(pred_y, train_y)\n",
    "            print(\"loss\", pred_y.float().mean(), train_y.float().mean(), loss)\n",
    "\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "            val_pred_y = self.model(val_X)\n",
    "            val_loss = self.loss_fn(val_pred_y, val_y)\n",
    "            \n",
    "            if batch_num % plot_cadence == 0:\n",
    "                train_loss_hist.append(loss.item())\n",
    "                val_loss_hist.append(val_loss.item())\n",
    "\n",
    "    return pd.DataFrame(dict(train=train_loss_hist, val=val_loss_hist))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
