{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "BATCH_SIZE = 8\n",
    "CONTEXT_LEN = 1024\n",
    "TEXT_FILE = \"./datasets/tinyshakespeare.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEXT_FILE, \"r\") as f:\n",
    "    data = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = torch.tensor(tokenizer.encode(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning scheme from \n",
    "# https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#partitioning-sequences\n",
    "d = torch.randint(CONTEXT_LEN, size = (1,))[0].item()\n",
    "\n",
    "# discard last item which may be of diff size\n",
    "token_partitons = torch.stack(torch.split(tokenized_data[d:], CONTEXT_LEN)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = token_partitons[:BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do we need a max_norm? seems like this would be important\n",
    "# depending on positional embedding scheme\n",
    "embedder = nn.Embedding(\n",
    "    num_embeddings = tokenizer.n_vocab,\n",
    "    embedding_dim = EMBEDDING_DIM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use learned positional embeddings for simplicity\n",
    "# TODO what are the tradeoffs with fixed positional embeddings besides less storage?\n",
    "positional_embedder = nn.Embedding(\n",
    "    num_embeddings = CONTEXT_LEN,\n",
    "    embedding_dim = EMBEDDING_DIM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_idx_tensor = torch.tensor(list(range(CONTEXT_LEN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_embeddings = positional_embedder(context_idx_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = embedder(sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = token_embeddings + positional_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _masked_softmax(self, logits, mask):\n",
    "        logits.masked_fill_(mask == 0, float('-inf'))\n",
    "        return self.softmax(logits)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        assert queries.shape == keys.shape\n",
    "        assert queries.shape[1] == values.shape[1]\n",
    "\n",
    "        ctx_len = queries.shape[1]\n",
    "        qk_dim = queries.shape[2]\n",
    "\n",
    "        # transpose (ctx, embedding) dims\n",
    "        scaled_dot_prod = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(qk_dim)\n",
    "\n",
    "        # TODO autoregressive mask; generalize this\n",
    "        mask = torch.tril(torch.ones(ctx_len, ctx_len))  \n",
    "\n",
    "        attention_weights = self._masked_softmax(scaled_dot_prod, mask)\n",
    "\n",
    "        # The book uses dropout for weights but doesn't explain why that specifically?\n",
    "        # https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#scaled-dot-product-attention\n",
    "        return torch.bmm(self.dropout(attention_weights), values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, qkv_dim, dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        assert qkv_dim % num_heads == 0\n",
    "\n",
    "        self.qkv_dim = qkv_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention_heads = [\n",
    "            Attention(dropout = dropout)\n",
    "            for _ in range(num_heads)\n",
    "        ]\n",
    "\n",
    "        self.q_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.k_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.v_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.output_layer = nn.LazyLinear(qkv_dim)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        assert queries.shape == keys.shape\n",
    "        assert queries.shape == values.shape\n",
    "        assert queries.shape[-1] == self.qkv_dim\n",
    "\n",
    "        head_dim = self.qkv_dim // self.num_heads\n",
    "        # TODO these are just truncated in the book?!\n",
    "        # https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#implementation\n",
    "        # why not use a linear layer to learn a lower dim value?\n",
    "        pq = torch.split(self.q_linear(queries), head_dim, dim=-1)\n",
    "        pk = torch.split(self.k_linear(keys), head_dim, dim=-1)\n",
    "        pv = torch.split(self.v_linear(values), head_dim, dim=-1)\n",
    "\n",
    "        # TODO use transpose trick to parallelize across heads\n",
    "        head_vals = [\n",
    "            self.attention_heads[i](pq[i], pk[i], pv[i])\n",
    "            for i in range(self.num_heads)\n",
    "        ]\n",
    "\n",
    "        cat_head_vals = torch.cat(head_vals, dim=-1)\n",
    "        return self.output_layer(cat_head_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(8, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha(embeddings, embeddings, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
