{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "BATCH_SIZE = 8\n",
    "CONTEXT_LEN = 1024\n",
    "TEXT_FILE = \"./datasets/tinyshakespeare.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEXT_FILE, \"r\") as f:\n",
    "    data = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = torch.tensor(tokenizer.encode(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning scheme from \n",
    "# https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#partitioning-sequences\n",
    "d = torch.randint(CONTEXT_LEN, size = (1,))[0].item()\n",
    "\n",
    "# discard last item which may be of diff size\n",
    "token_partitons = torch.stack(torch.split(tokenized_data[d:], CONTEXT_LEN)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = token_partitons[:BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do we need a max_norm? seems like this would be important\n",
    "# depending on positional embedding scheme\n",
    "embedder = nn.Embedding(\n",
    "    num_embeddings = tokenizer.n_vocab,\n",
    "    embedding_dim = EMBEDDING_DIM\n",
    ")\n",
    "# use learned positional embeddings for simplicity\n",
    "# TODO what are the tradeoffs with fixed positional embeddings besides less storage?\n",
    "positional_embedder = nn.Embedding(\n",
    "    num_embeddings = CONTEXT_LEN,\n",
    "    embedding_dim = EMBEDDING_DIM\n",
    ")\n",
    "context_idx_tensor = torch.tensor(list(range(CONTEXT_LEN)))\n",
    "positional_embeddings = positional_embedder(context_idx_tensor)\n",
    "token_embeddings = embedder(sample_batch)\n",
    "embeddings = token_embeddings + positional_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _masked_softmax(self, logits, mask):\n",
    "        logits.masked_fill_(mask == 0, float('-inf'))\n",
    "        return self.softmax(logits)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        assert queries.shape == keys.shape\n",
    "        assert queries.shape[1] == values.shape[1]\n",
    "\n",
    "        ctx_len = queries.shape[1]\n",
    "        qk_dim = queries.shape[2]\n",
    "\n",
    "        # transpose (ctx, embedding) dims\n",
    "        scaled_dot_prod = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(qk_dim)\n",
    "\n",
    "        # TODO autoregressive mask; generalize this\n",
    "        mask = torch.tril(torch.ones(ctx_len, ctx_len))  \n",
    "\n",
    "        attention_weights = self._masked_softmax(scaled_dot_prod, mask)\n",
    "\n",
    "        # The book uses dropout for weights but doesn't explain why that specifically?\n",
    "        # https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#scaled-dot-product-attention\n",
    "        return torch.bmm(self.dropout(attention_weights), values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, qkv_dim, dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        assert qkv_dim % num_heads == 0\n",
    "\n",
    "        self.qkv_dim = qkv_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention_heads = [\n",
    "            Attention(dropout = dropout)\n",
    "            for _ in range(num_heads)\n",
    "        ]\n",
    "\n",
    "        self.q_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.k_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.v_linear = nn.LazyLinear(qkv_dim)\n",
    "        self.output_layer = nn.LazyLinear(qkv_dim)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        assert queries.shape == keys.shape\n",
    "        assert queries.shape == values.shape\n",
    "        assert queries.shape[-1] == self.qkv_dim\n",
    "\n",
    "        head_dim = self.qkv_dim // self.num_heads\n",
    "        # TODO these are just truncated in the book?!\n",
    "        # https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html#implementation\n",
    "        # why not use a linear layer to learn a lower dim value?\n",
    "        pq = torch.split(self.q_linear(queries), head_dim, dim=-1)\n",
    "        pk = torch.split(self.k_linear(keys), head_dim, dim=-1)\n",
    "        pv = torch.split(self.v_linear(values), head_dim, dim=-1)\n",
    "\n",
    "        # TODO use transpose trick to parallelize across heads\n",
    "        head_vals = [\n",
    "            self.attention_heads[i](pq[i], pk[i], pv[i])\n",
    "            for i in range(self.num_heads)\n",
    "        ]\n",
    "\n",
    "        cat_head_vals = torch.cat(head_vals, dim=-1)\n",
    "        return self.output_layer(cat_head_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFFN(nn.Sequential):\n",
    "    def __init__(self, ffn_hidden_dim: int, ffn_output_dim: int):\n",
    "        super().__init__(\n",
    "            nn.LazyLinear(ffn_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(ffn_output_dim)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, norm_shape, dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(norm_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(X + self.dropout(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads: int, model_dim: int, ffn_hidden_dim: int, dropout = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(num_heads, model_dim, dropout)\n",
    "        self.add_norm1 = AddNorm(model_dim, dropout)\n",
    "        self.ffn = PositionwiseFFN(ffn_hidden_dim, model_dim)\n",
    "        self.add_norm2 = AddNorm(model_dim, dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        attention = self.attention(X, X, X)\n",
    "        resid_out = self.add_norm1(X, attention)\n",
    "\n",
    "        ffn_out = self.ffn(resid_out)\n",
    "        return self.add_norm2(resid_out, ffn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RalphGPT(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks: int,\n",
    "        num_heads: int,\n",
    "        model_dim: int,\n",
    "        ffn_hidden_dim: int,\n",
    "        vocab_size: int,\n",
    "        context_len: int,\n",
    "        dropout = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_len = context_len\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        # TODO do we need a max_norm? seems like this would be important\n",
    "        # depending on positional embedding scheme\n",
    "        self.token_embedder = nn.Embedding(\n",
    "            num_embeddings = vocab_size,\n",
    "            embedding_dim = model_dim\n",
    "        )\n",
    "\n",
    "        # use learned positional embeddings for simplicity\n",
    "        # TODO what are the tradeoffs with fixed positional embeddings besides less storage?\n",
    "        self.positional_embedder = nn.Embedding(\n",
    "            num_embeddings = context_len,\n",
    "            embedding_dim = model_dim\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(*[\n",
    "            TransformerDecoderBlock(num_heads, model_dim, ffn_hidden_dim, dropout)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.output = nn.LazyLinear(vocab_size)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens.shape is (batch size, ctx len)\n",
    "        assert tokens.shape[-1] == self.context_len\n",
    "\n",
    "        context_idx_tensor = torch.tensor(list(range(self.context_len)))\n",
    "        positional_embeddings = self.positional_embedder(context_idx_tensor)\n",
    "\n",
    "        token_embeddings = embedder(tokens)\n",
    "        embeddings = token_embeddings + positional_embeddings\n",
    "\n",
    "        return self.output(self.decoder(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ralph/.venvs/d2dl/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "ralphgpt = RalphGPT(\n",
    "    num_blocks = 10,\n",
    "    num_heads = 4,\n",
    "    model_dim = EMBEDDING_DIM,\n",
    "    ffn_hidden_dim = EMBEDDING_DIM,\n",
    "    vocab_size = tokenizer.n_vocab,\n",
    "    context_len = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ralphgpt(sample_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
